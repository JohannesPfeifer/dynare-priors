\documentclass{amsart}

\usepackage[utf8]{inputenc}


\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

\numberwithin{equation}{section}

\newcommand{\Dynare}{\textsc{Dynare}}

\setlength\parindent{0pt} % No identation in all document.

\begin{document}

\title{Prior distributions in Dynare}
\author{Stéphane Adjemian}
\address{Université du Maine}
\email{stepan@dynare.org}
\date{April, 2016}

\maketitle

\section{Gamma distributions}\label{sec:GammaDistribution}

\begin{definition}
  The Gamma function is defined as follows:
  \[
    \Gamma(n) = \int_0^{\infty}x^{n-1}e^{-x}\mathrm d x
  \]
  for any $x>0$, zero elsewhere.
\end{definition}

One can easily prove that the foillowing identities hold:
$\Gamma(1) = 1$, $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ and
$\Gamma(n)=(n-1)\Gamma(n-1)$.

\begin{definition}
  A positive real random variable has a gamma distribution with
  parameters $\alpha>0$ (shape) and $\beta>0$ (scale) if and only of
  its probability density function is given by the following equation:
  \[
    f(x)=\mathcal C(\alpha,\beta)^{-1} \times x^{\alpha-1}e^{-\frac{x}{\beta}}
  \]
  where $\mathcal C(\alpha, \beta) = \Gamma(\alpha)\beta^{\alpha}$ is the constant of
  integration.
\end{definition}
 We will denote $X\sim G(\alpha, \beta)$.  In Dynare this
 distribution may be specified as a prior in the estimated params
 block (using the keyword \verb+GAMMA_PDF+). The user has to specify the
 expectation and standard deviation of the distribution.

 \begin{proposition}
   \label{GammaDistributionMoments}
   If $X\sim G(\alpha, \beta)$, then the expectation and variance of $X$ are:
   \[
     \begin{split}
       \mu &= \alpha\beta \\
       \sigma^2 &= \alpha\beta^2
     \end{split}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistributionMoments}]
   By definition the expectation is given by:
   \[
     \begin{split}
       \mu &= \mathcal C(\alpha,\beta)^{-1}\int_{0}^{\infty}x^{\alpha}e^{-\frac{x}{\beta}}\mathrm d x \\
       &= \mathcal C(\alpha,\beta)^{-1}\mathcal C(1+\alpha,\beta) \\
       &= \frac{\Gamma(1+\alpha)\beta^{1+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \frac{\alpha\Gamma(\alpha)\beta^{1+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \alpha\beta
     \end{split}
   \]
   The second order moment is given by:
   \[
     \begin{split}
       \mathbb E [X^2] &= \mathcal C(\alpha,\beta)^{-1}\int_{0}^{\infty}x^{\alpha+1}e^{-\frac{x}{\beta}}\mathrm d x \\
       &= \mathcal C(\alpha,\beta)^{-1}\mathcal C(2+\alpha,\beta) \\
       &= \frac{(\alpha+1)\alpha\Gamma(\alpha)\beta^{2+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \alpha(\alpha+1)\beta^2 
     \end{split}
   \]
   and the variance:
   \[
     \mathbb V[X] = \alpha(\alpha+1)\beta^2 - \alpha^2\beta^2 = \alpha\beta^2
   \]
 \end{proof}
 
 \begin{remark}
   Dynare computes the hyperparameters $\alpha$ and $\beta$ by
   inverting the formulas given in proposition
   \ref{GammaDistributionMoments}, we have:
   \[
     \begin{split}
       \alpha &= \frac{\mu^2}{\sigma^2} \\
       \beta &= \frac{\sigma^2}{\mu}
     \end{split}
   \]
 \end{remark}

 \begin{proposition}
   \label{GammaDistributionMode}
   The mode of $X\sim G(\alpha, \beta)$ is:
   \[
     m = 
     \begin{cases}
       0 &\text{if } \alpha\leq 1\text{,} \\
       (\alpha-1)\beta &\text{otherwise.}
     \end{cases}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistributionMode}]
   We have:
   \[
     \begin{split}
     \frac{\mathrm d}{\mathrm dx} f(x) &= \mathcal{C}(\alpha,\beta)^{-1}
     \times
     \left\{
         (\alpha-1)x^{\alpha-2}e^{-\frac{x}{\beta}}-\frac{1}{\beta}x^{\alpha-1}e^{-\frac{x}{\beta}}
     \right\} \\
      &= \mathcal{C}(\alpha,\beta)^{-1} x^{\alpha-1}e^{-\frac{x}{\beta}}
      \times
      \left\{ -\frac{1-\alpha}{x}-\frac{1}{\beta} \right\} \\
     \end{split}
   \]
   If $\alpha\leq 1$ the density is monotone decreasing and we have a vertical asymptote at zero. Consequently, if $\alpha\leq 1$ we must have $m=0$. If the shape parameter is greater than one, the previous derivative is non negative if and only if:
   \[
     -\frac{1-\alpha}{x}-\frac{1}{\beta}\geq 0
   \]
   or equivalently:
   \[
     x \leq (\alpha-1)\beta
   \]
   If the shape parameter is greater than one, $\alpha>1$, we have $m=(\alpha-1)\beta$. If the shape parameter is equal to one, the mode is also zero but without vertical asymptot (the slope of the density is null at zero). 
 \end{proof}

 \begin{remark}
   With an asymmetric distribution one may prefer to define the hyperparameters from the mode and the variance (instead of the mean and the variance). Assuming that $\alpha>1$, we have:
   \[
     \begin{cases}
       m &= (\alpha-1)\beta\\
       \sigma^2 &= \alpha\beta^2
     \end{cases}
     \Leftrightarrow
     \begin{cases}
       \alpha &= 1+\frac{m}{\beta} \\
       0 &= \beta^2+m\beta-\sigma^2
     \end{cases}
   \]
   The quadratic equation has always two distinct real solutions of opposite sign. Selecting the positive solution (because the scale parameter $\beta$ has to be positive), we obtain:
   \[
     \begin{split}
       \alpha &= 1 - \frac{2}{1-\sqrt{1+4\left(\frac{\sigma}{m}\right)^2}}\\
       \beta &= -\frac{m}{2}\left(1-\sqrt{1+4\left(\frac{\sigma}{m}\right)^2}\right)
     \end{split}
   \]
   If the shape parameter is not greater than one, $\alpha\leq 1$, then we cannot identify uniquely the two hyperparameters from the mode and the variance (we have a continuum of possible values for $\alpha$ and $\beta$).
 \end{remark}


\subsection{Gamma types 2 and 1 distributions}\label{subsec:GammaDistributionTypes}

\begin{definition}
  Let $X>0$ gamma random variable parameterized by a shape $\frac{\nu}{2}>0$ and a scale $\frac{2}{s}>0$. We will denote $X\sim G_2(\nu, s) \equiv G\left(\frac{\nu}{2}, \frac{2}{s}\right)$, and say that $X$ has a gamma-2 distribution. 
\end{definition}

\begin{definition}
  Let $Y = \sqrt{X}$ with $X\sim G_2(\nu, s)$, we say that $Y$ has a gamma-1 distribution, $Y\sim G_1(\nu, s)$. 
\end{definition}

These distributions are not implemented in Dynare but can be easily built from the gamma distribution.

 \begin{proposition}
   \label{GammaDistribution:type1and2:densities}
   The densities of the gamma-2 and gamma-1 are respectively:
   \[
     f_X(x) = \mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}x^{\frac{\nu}{2}-1}e^{-\frac{sx}{2}}
   \]
   and
   \[
     f_Y(y) = \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-1}e^{-\frac{s}{2}y^2}
   \]
   where $\widetilde{\mathcal C}(\alpha,\beta) = \frac{1}{2}\mathcal C(\alpha,\beta)$.
 \end{proposition}

\begin{proof}[Proof of proposition \ref{GammaDistribution:type1and2:densities}]
The density of the gamma-2 distribution is easily obtained from the density of the gamma distribution, and the first two moments of this distribution are given by proposition \ref{GammaDistributionMoments}. The density of the gamma-1 distribution is given by:
\[
f_Y(y) = f_X\left(h^{-1}(y)\right)\times \left|\frac{\mathrm d}{\mathrm d y}h^{-1}(y)\right|
\]
by applying the change of variable formula, where $h(x) = \sqrt{x}$, and $f_X(x)$ is the density of the gamma-2 distribution. Substituting $f_X$ and the reciprocal of $h$ in the definition of $f_Y$, we get:
\[
  \begin{split}
    f_Y(y) &= \mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-2}e^{-\frac{s}{2}y^2}\times \left| \frac{\mathrm d}{\mathrm d y}y^2\right|\\
    \Leftrightarrow f_Y(y) &= 2\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-1}e^{-\frac{s}{2}y^2}
  \end{split}
\]
with $2\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1} = \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}$.
\end{proof}

\begin{proposition}
   \label{GammaDistribution:type2:moments}
   If $X\sim G_2(\nu, s)$, then the expectation and variance of $X$ are:
   \[
     \mu = \frac{\nu}{s}
   \]
   \[
     \sigma^2 = \frac{2\nu}{s^2}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistribution:type2:moments}]
   Direct by substituting the definition of the gamma-2 distribution in proposition \ref{GammaDistributionMoments}.
 \end{proof}

\begin{proposition}
   \label{GammaDistribution:type1:moments}
   If $X\sim G_1(\nu, s)$, then the expectation and variance of $X$ are:
   \[
     \mu = \sqrt{\frac{\nu}{s}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}
   \]
   \[
     \sigma^2 = \frac{\nu}{s} - \mu^2
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistribution:type1:moments}]
By definition, we have:
\[
  \begin{split}
    \mu &= \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}\int_0^{\infty}y^{\nu}e^{-\frac{s}{2}y^2}\mathrm dy\\
    &=\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}\int_{0}^{\infty}x^{\frac{\nu}{2}-\frac{1}{2}}e^{-\frac{s}{2}x}\mathrm d x\\
    &= \frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}+\frac{1}{2}}}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}}}
  \end{split}
\]
So that
\[
\mu = \sqrt{\frac{\nu}{s}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}
\]
For the second order moment, we have by definitions of the gamma-1 and gamma-2 distributions:
\[
\mathbb E\left[Y^2\right] = \mathbb E [X]
\]
where $X~G_2(\nu, s)$. From proposition \ref{GammaDistribution:type2:moments}, we obtain:
\[
\mathbb E\left[Y^2\right] = \frac{\nu}{s}
\]
 \end{proof}

 \subsection{Chi-squared and Exponential distributions}\label{subsec:ChiAndExponentialDistributions}
 A number of distributions may be defined as special cases of the
 Gamma distribution. A chi-squared distribution with $\nu$ degrees of
 freedom, $\chi^2(\nu)$, is a gamma distribution:
 $G\left(\frac{\nu}{2},2\right)$. The chi-squared prior is not
 implemented in dynare but obviously the user can obtain it by
 carefully choosing the expectation and variance of the gamma prior
 (that is, by setting $\mu=\nu$ and $\sigma^2 = 2\nu$). As long as the
 variance is twice the expectation, the prior is a chi-squared
 distribution. An exponential distribution with expectation
 $\lambda^{-1}$, $\xi(\lambda)$, is also a gamma distribution:
 $G\left(1,\frac{1}{\lambda}\right)$. Again the exponential prior is
 not implemented in dynare but is obtained from the gamma distribution
 as long as the prior expectation is the squared root of the prior
 variance. As a consequence, by using the gamma prior and setting
 $\mu=\sigma$ the user chooses a distribution whose mode is zero.

 \subsection{Shifted Gamma distribution}\label{subsec:ShiftedGammaDistribution}
 The support of the gamma distribution is by default the positive real
 line. In \Dynare\ the user has the possibility to shift the support
 if this distribution. This may be useful, for instance, if someone
 wants to estimate the elasticity of substitution of a CES production
 function with the (deterministic) belief that this elasticity has to
 be greater than one (Cobb-Douglas technology). The density is then
 defined with three parameters $\alpha>0$ (shape), $\beta>0$ (scale)
 and $\delta\in\mathbb R$ (location, the lower bound of the
 distribution’s support):
 \[
   f(x) = \mathcal C(\alpha, \beta)^{-1}(x-\delta)^{\alpha-1}e^{-\frac{x-\delta}{\beta}}
 \]
 where tghe constant of integration is defined as before. Obviously
 this shift affects the first order moment ($\mu=\alpha\beta+\delta$)
 and the mode (the same shift applies) but not the variance.\newline


\end{document}