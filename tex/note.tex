% © 2017 Stéphane Adjemian <stephane.adjemian@univ-lemans.fr>
%
% This text is licensed under CC Attribution-ShareAlike 4.0 International.

\documentclass{amsart}

\usepackage[utf8]{inputenc}

\usepackage{nicefrac}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

\numberwithin{equation}{section}

\newcommand{\Dynare}{\textsc{Dynare}}

\setlength\parindent{0pt} % No identation in all document.

\begin{document}

\title{Prior distributions in Dynare}
\author{Stéphane Adjemian}
\address{Université du Maine}
\email{stepan@dynare.org}
\date{April, 2016}

\maketitle

\section{Gamma distributions}\label{sec:GammaDistribution}

\begin{definition}
  The Gamma function is defined as follows:
  \[
    \Gamma(n) = \int_0^{\infty}x^{n-1}e^{-x}\mathrm d x
  \]
  for any $x>0$, zero elsewhere.
\end{definition}

One can easily prove that the foillowing identities hold:
$\Gamma(1) = 1$, $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ and
$\Gamma(n)=(n-1)\Gamma(n-1)$.

\begin{definition}
  A positive real random variable has a gamma distribution with
  parameters $\alpha>0$ (shape) and $\beta>0$ (scale) if and only of
  its probability density function is given by the following equation:
  \[
    f(x)=\mathcal C(\alpha,\beta)^{-1} \times x^{\alpha-1}e^{-\frac{x}{\beta}}
  \]
  where $\mathcal C(\alpha, \beta) = \Gamma(\alpha)\beta^{\alpha}$ is the constant of
  integration.
\end{definition}
 We will denote $X\sim G(\alpha, \beta)$.  In Dynare this
 distribution may be specified as a prior in the estimated params
 block (using the keyword \verb+GAMMA_PDF+). The user has to specify the
 expectation and standard deviation of the distribution.

 \begin{proposition}
   \label{GammaDistributionMoments}
   If $X\sim G(\alpha, \beta)$, then the expectation and variance of $X$ are:
   \[
     \begin{split}
       \mu &= \alpha\beta \\
       \sigma^2 &= \alpha\beta^2
     \end{split}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistributionMoments}]
   By definition the expectation is given by:
   \[
     \begin{split}
       \mu &= \mathcal C(\alpha,\beta)^{-1}\int_{0}^{\infty}x^{\alpha}e^{-\frac{x}{\beta}}\mathrm d x \\
       &= \mathcal C(\alpha,\beta)^{-1}\mathcal C(1+\alpha,\beta) \\
       &= \frac{\Gamma(1+\alpha)\beta^{1+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \frac{\alpha\Gamma(\alpha)\beta^{1+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \alpha\beta
     \end{split}
   \]
   The second order moment is given by:
   \[
     \begin{split}
       \mathbb E [X^2] &= \mathcal C(\alpha,\beta)^{-1}\int_{0}^{\infty}x^{\alpha+1}e^{-\frac{x}{\beta}}\mathrm d x \\
       &= \mathcal C(\alpha,\beta)^{-1}\mathcal C(2+\alpha,\beta) \\
       &= \frac{(\alpha+1)\alpha\Gamma(\alpha)\beta^{2+\alpha}}{\Gamma(\alpha)\beta^{\alpha}} \\
       &= \alpha(\alpha+1)\beta^2
     \end{split}
   \]
   and the variance:
   \[
     \mathbb V[X] = \alpha(\alpha+1)\beta^2 - \alpha^2\beta^2 = \alpha\beta^2
   \]
 \end{proof}

 \begin{remark}
   Dynare computes the hyperparameters $\alpha$ and $\beta$ by
   inverting the formulas given in proposition
   \ref{GammaDistributionMoments}, we have:
   \[
     \begin{split}
       \alpha &= \frac{\mu^2}{\sigma^2} \\
       \beta &= \frac{\sigma^2}{\mu}
     \end{split}
   \]
 \end{remark}

 \begin{proposition}
   \label{GammaDistributionMode}
   The mode of $X\sim G(\alpha, \beta)$ is:
   \[
     m =
     \begin{cases}
       0 &\text{if } \alpha\leq 1\text{,} \\
       (\alpha-1)\beta &\text{otherwise.}
     \end{cases}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistributionMode}]
   We have:
   \[
     \begin{split}
     \frac{\mathrm d}{\mathrm dx} f(x) &= \mathcal{C}(\alpha,\beta)^{-1}
     \times
     \left\{
         (\alpha-1)x^{\alpha-2}e^{-\frac{x}{\beta}}-\frac{1}{\beta}x^{\alpha-1}e^{-\frac{x}{\beta}}
     \right\} \\
      &= \mathcal{C}(\alpha,\beta)^{-1} x^{\alpha-1}e^{-\frac{x}{\beta}}
      \times
      \left\{ -\frac{1-\alpha}{x}-\frac{1}{\beta} \right\} \\
     \end{split}
   \]
   If $\alpha\leq 1$ the density is monotone decreasing and we have a vertical asymptote at zero. Consequently, if $\alpha\leq 1$ we must have $m=0$. If the shape parameter is greater than one, the previous derivative is non negative if and only if:
   \[
     -\frac{1-\alpha}{x}-\frac{1}{\beta}\geq 0
   \]
   or equivalently:
   \[
     x \leq (\alpha-1)\beta
   \]
   If the shape parameter is greater than one, $\alpha>1$, we have $m=(\alpha-1)\beta$. If the shape parameter is equal to one, the mode is also zero but without vertical asymptot (the slope of the density is null at zero).
 \end{proof}

 \begin{remark}
   With an asymmetric distribution one may prefer to define the hyperparameters from the mode and the variance (instead of the mean and the variance). Assuming that $\alpha>1$, we have:
   \[
     \begin{cases}
       m &= (\alpha-1)\beta\\
       \sigma^2 &= \alpha\beta^2
     \end{cases}
     \Leftrightarrow
     \begin{cases}
       \alpha &= 1+\frac{m}{\beta} \\
       0 &= \beta^2+m\beta-\sigma^2
     \end{cases}
   \]
   The quadratic equation has always two distinct real solutions of opposite sign. Selecting the positive solution (because the scale parameter $\beta$ has to be positive), we obtain:
   \[
     \begin{split}
       \alpha &= 1 - \frac{2}{1-\sqrt{1+4\left(\frac{\sigma}{m}\right)^2}}\\
       \beta &= -\frac{m}{2}\left(1-\sqrt{1+4\left(\frac{\sigma}{m}\right)^2}\right)
     \end{split}
   \]
   If the shape parameter is not greater than one, $\alpha\leq 1$, then we cannot identify uniquely the two hyperparameters from the mode and the variance (we have a continuum of possible values for $\alpha$ and $\beta$).
 \end{remark}


\subsection{Gamma distributions of types I and II}\label{subsec:GammaDistributionTypes}

\begin{definition}
  Let $X>0$ gamma random variable parameterized by a shape
  $\frac{\nu}{2}>0$ and a scale $\frac{2}{s}>0$. We will denote
  $X\sim G_2(\nu, s) \equiv G\left(\frac{\nu}{2}, \frac{2}{s}\right)$,
  and say that $X$ has a gamma distribution of types II.
\end{definition}

\begin{definition}
  Let $Y = \sqrt{X}$ with $X\sim G_2(\nu, s)$, we say that $Y$ has a
  gamma distribution of type I, $Y\sim G_1(\nu, s)$.
\end{definition}

These distributions are not implemented in Dynare but can be easily built from the gamma distribution.

 \begin{proposition}
   \label{GammaDistribution:type1and2:densities}
   The densities of the gamma distributions of type II and I are respectively:
   \[
     f_X(x) = \mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}x^{\frac{\nu}{2}-1}e^{-\frac{sx}{2}}
   \]
   and
   \[
     f_Y(y) = \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-1}e^{-\frac{s}{2}y^2}
   \]
   where $\widetilde{\mathcal C}(\alpha,\beta) = \frac{1}{2}\mathcal C(\alpha,\beta)$.
 \end{proposition}

\begin{proof}[Proof of proposition \ref{GammaDistribution:type1and2:densities}]
  The density of the gamma distribution of type II is easily obtained
  from the density of the gamma distribution, and the first two
  moments of this distribution are given by proposition
  \ref{GammaDistributionMoments}. The density of the gamma
  distribution of type I is given by:
  \[
    f_Y(y) = f_X\left(h^{-1}(y)\right)\times \left|\frac{\mathrm d}{\mathrm d y}h^{-1}(y)\right|
  \]
  by applying the change of variable formula, where $h(x) = \sqrt{x}$,
  and $f_X(x)$ is the density of the gamma
  distribution of type II. Substituting $f_X$ and the reciprocal of $h$ in the
  definition of $f_Y$, we get:
  \[
    \begin{split}
      f_Y(y) &= \mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-2}e^{-\frac{s}{2}y^2}\times \left| \frac{\mathrm d}{\mathrm d y}y^2\right|\\
      \Leftrightarrow f_Y(y) &= 2\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}y^{\nu-1}e^{-\frac{s}{2}y^2}
    \end{split}
  \]
  with $2\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1} = \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}$.
\end{proof}

\begin{proposition}
   \label{GammaDistribution:type2:moments}
   If $X\sim G_2(\nu, s)$, then the expectation and variance of $X$ are:
   \[
     \mu = \frac{\nu}{s}
   \]
   \[
     \sigma^2 = \frac{2\nu}{s^2}
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistribution:type2:moments}]
   Direct by substituting the definition of the gamma distribution of type II in proposition \ref{GammaDistributionMoments}.
 \end{proof}

\begin{proposition}
   \label{GammaDistribution:type1:moments}
   If $X\sim G_1(\nu, s)$, then the expectation and variance of $X$ are:
   \[
     \mu = \sqrt{\frac{\nu}{s}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}
   \]
   \[
     \sigma^2 = \frac{\nu}{s} - \mu^2
   \]
 \end{proposition}

 \begin{proof}[Proof of proposition \ref{GammaDistribution:type1:moments}]
By definition, we have:
\[
  \begin{split}
    \mu &= \widetilde{\mathcal C}\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}\int_0^{\infty}y^{\nu}e^{-\frac{s}{2}y^2}\mathrm dy\\
    &=\mathcal C\left(\frac{\nu}{2},\frac{2}{s}\right)^{-1}\int_{0}^{\infty}x^{\frac{\nu}{2}-\frac{1}{2}}e^{-\frac{s}{2}x}\mathrm d x\\
    &= \frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}+\frac{1}{2}}}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}}}
  \end{split}
\]
So that
\[
\mu = \sqrt{\frac{\nu}{s}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}
\]
For the second order moment, we have by definitions of the gamma distributions of types I and II:
\[
\mathbb E\left[Y^2\right] = \mathbb E [X]
\]
where $X~G_2(\nu, s)$. From proposition \ref{GammaDistribution:type2:moments}, we obtain:
\[
\mathbb E\left[Y^2\right] = \frac{\nu}{s}
\]
 \end{proof}

 \subsection{Chi-squared and Exponential distributions}\label{subsec:ChiAndExponentialDistributions}
 A number of distributions may be defined as special cases of the
 Gamma distribution. A chi-squared distribution with $\nu$ degrees of
 freedom, $\chi^2(\nu)$, is a gamma distribution:
 $G\left(\frac{\nu}{2},2\right)$. The chi-squared prior is not
 implemented in dynare but obviously the user can obtain it by
 carefully choosing the expectation and variance of the gamma prior
 (that is, by setting $\mu=\nu$ and $\sigma^2 = 2\nu$). As long as the
 variance is twice the expectation, the prior is a chi-squared
 distribution. An exponential distribution with expectation
 $\lambda^{-1}$, $\xi(\lambda)$, is also a gamma distribution:
 $G\left(1,\frac{1}{\lambda}\right)$. Again the exponential prior is
 not implemented in dynare but is obtained from the gamma distribution
 as long as the prior expectation is the squared root of the prior
 variance. As a consequence, by using the gamma prior and setting
 $\mu=\sigma$ the user chooses a distribution whose mode is zero.

 \subsection{Shifted Gamma distribution}\label{subsec:ShiftedGammaDistribution}
 The support of the gamma distribution is by default the positive real
 line. In \Dynare\ the user has the possibility to shift the support
 if this distribution. This may be useful, for instance, if someone
 wants to estimate the elasticity of substitution of a CES production
 function with the (deterministic) belief that this elasticity has to
 be greater than one (Cobb-Douglas technology). The density is then
 defined with three parameters $\alpha>0$ (shape), $\beta>0$ (scale)
 and $\delta\in\mathbb R$ (location, the lower bound of the
 distribution’s support):
 \[
   f(x) = \mathcal C(\alpha, \beta)^{-1}(x-\delta)^{\alpha-1}e^{-\frac{x-\delta}{\beta}}
 \]
 where the constant of integration is defined as before. Obviously
 this shift affects the first order moment ($\mu=\alpha\beta+\delta$)
 and the mode (the same shift applies) but not the variance.\newline

\section{Inverted Gamma distributions}\label{sec:InvertedGammaDistributions}

\begin{definition}
  Let $X$ be a gamma distributed random variable with shape parameter
  $\alpha>0$ and scale parameter $\beta>0$. Then $Z = X^{-1}$ has an
  inverted gamma inverted distribution, $Z\sim IG(\alpha, \beta)$.
\end{definition}

\begin{proposition}\label{InvertedGammaDensity}
  The density of the continuous random variable $Z\sim IG(\alpha,\beta)$ is:
  \[
    f(z) = \mathcal C (\alpha, \beta)^{-1}x^{-\alpha-1}e^{-\frac{1}{\beta z}}
  \]
  where $\mathcal (\alpha, \beta) = \Gamma(\alpha)\beta^{\alpha}$ is the constant of integration.
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGammaDensity}]
  Let $f_X$ denote the density of the gamma distribution with shape
  and scale parameters $\alpha>0$ and $\beta>0$, and define
  $h(x) = \frac{1}{x}$. The density of the inverted gamma distribution
  is defined as follows:
  \[
    \begin{split}
      f(z) &= f_X(h^{-1}(z)) \times \left|\frac{\mathrm d}{\mathrm dz}h^{-1}(z)\right|\\
      &= \mathcal C(\alpha, \beta)^{-1} \times z^{-\alpha+1}e^{-\frac{1}{\beta z}} \times \left|\frac{\mathrm d}{\mathrm dz}\frac{1}{z}\right|\\
      &= \mathcal C(\alpha, \beta)^{-1} z^{-\alpha-1}e^{-\frac{1}{\beta z}}
    \end{split}
  \]
\end{proof}

\begin{proposition}\label{InvertedGammaMoments}
  The expectation and variance of the continuous random variable $Z\sim IG(\alpha,\beta)$ are:
  \[
    \begin{split}
      \mu &= \frac{1}{\beta(\alpha-1)}\\
      \sigma^2 &= \frac{1}{\beta^2(\alpha-1)^2(\alpha-2)} \quad\text{for any }\alpha>2
    \end{split}
  \]
\end{proposition}

Note that the variance is not defined for $\alpha<2$, when equality
holds the variance is infinite.\newline

\begin{proof}[Proof of proposition \ref{InvertedGammaMoments}]
  By definition of the probability density function of an Inverted Gamma distribution, we have:
  \[
    \begin{split}
      \mu &= \mathcal C(\alpha, \beta)^{-1} \int_0^{\infty}z^{-\alpha}e^{-\frac{1}{\beta z}}\mathrm dz\\
      &= \mathcal C(\alpha, \beta)^{-1} \int_0^{\infty}u^{\alpha-2}e^{-\frac{u}{\beta}}\mathrm du\\
      &= \mathcal C(\alpha, \beta)^{-1} \mathcal C(\alpha-1, \beta)\\
      &= \frac{\Gamma(\alpha-1)\beta^{\alpha-1}}{\Gamma(\alpha)\beta^{\alpha}}\\
      &= \frac{\Gamma(\alpha-1)\beta^{\alpha-1}}{(\alpha-1)\Gamma(\alpha-1)\beta^{\alpha}}\\
      &= \frac{1}{(\alpha-1)\beta}
    \end{split}
  \]
  and
  \[
    \begin{split}
      \mathbb E[Z^2] &= \mathcal C(\alpha, \beta)^{-1} \int_0^{\infty}z^{-\alpha+1}e^{-\frac{1}{\beta z}}\mathrm dz\\
      &= \mathcal C(\alpha, \beta)^{-1} \int_0^{\infty}u^{\alpha-3}e^{-\frac{u}{\beta}}\mathrm du\\
      &= \mathcal C(\alpha, \beta)^{-1} \mathcal C(\alpha-2, \beta)\\
      &= \frac{\Gamma(\alpha-2)\beta^{\alpha-2}}{\Gamma(\alpha)\beta^{\alpha}}\\
      &= \frac{\Gamma(\alpha-2)\beta^{\alpha-2}}{(\alpha-2)(\alpha-1)\Gamma(\alpha-2)\beta^{\alpha}}\\
      &= \frac{1}{(\alpha-2)(\alpha-1)\beta^2}
    \end{split}
  \]
  so that
  \[
    \begin{split}
      \sigma^2 &= \frac{1}{(\alpha-2)(\alpha-1)\beta^2} - \frac{1}{(\alpha-1)^2\beta^2}\\
      &= \frac{\alpha-1}{(\alpha-2)(\alpha-1)^2\beta^2} - \frac{\alpha-2}{(\alpha-2)(\alpha-1)^2\beta^2}\\
      &= \frac{1}{\beta^2(\alpha-1)^2(\alpha-2)}
    \end{split}
  \]
\end{proof}

The system of equations defining the expectation and the variance can be solved
for the shape and scale parameters:

\begin{equation}
  \begin{split}
    \alpha &= 2 + \left(\frac{\mu}{\sigma}\right)^2\\
    \beta &= \frac{1}{\mu \left[1+\left(\frac{\mu}{\sigma}\right)^2\right]}
  \end{split}
\end{equation}

This distribution is not implemented in \Dynare, but two special cases presented in the next sub-section are implemented.\newline

\begin{proposition}\label{InvertedGammaMode}
  The mode of $Z\sim IG(\alpha,\beta)$ is strictly positive and smaller than the expectation:
  \[
    m = \frac{1}{\beta(1+\alpha)}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGammaMode}]
  We have:
  \[
    \frac{\mathrm d}{\mathrm dz}f(z) = \mathcal C(\alpha,\beta)^{-1}z^{-\alpha-2}e^{-\frac{1}{\beta z}}\left\{\frac{1}{\beta z}-(1+\alpha)\right\}
  \]
  There exists only one positive value for $z$ such that this first derivative is zero:
  \[
    \frac{\mathrm d}{\mathrm dz}f(z) = 0 \quad \Leftrightarrow \quad z = \frac{1}{\beta(1+\alpha)}
  \]
\end{proof}

Again one may prefer to define this distribution by specifying the mode and the
variance (see below for the inverse gamma distribution of type II). Note that it is also
possible to define this distribution by specifying the expectation and the mode. We have:

\[
  \begin{cases}
    \mu &= \frac{1}{\beta(\alpha-1)}\\
    m &= \frac{1}{\beta(1+\alpha)}
  \end{cases}
\]

or equivalently

\[
  \begin{cases}
    \mu &= \frac{1}{\beta(\alpha-1)}\\
    m &= \mu\frac{\alpha-1}{\alpha+1}
  \end{cases}
\]

Solving the second equation for $\alpha$ we obtain:

\[
\alpha = \left(1-\frac{m}{\mu}\right)^{-1}\left(\frac{m}{\mu}+1\right) = \frac{\mu+m}{\mu-m}
\]

and by substitution in the first equation:

\[
\beta = \frac{1}{2}\left(\frac{1}{m}-\frac{1}{\mu}\right) = \frac{\mu-m}{2\mu m}
\]

\subsection{Inverted Gamma of types I and II}\label{sec:InvertedGammaDistributionsOfType1And2}

\begin{definition}\label{InvertedGamma2}
  Let $X>0$ be a real random variable with gamma distribution of type II,
  $X\sim G\left(\frac{\nu}{2},\frac{2}{s}\right)$. $Y = X^{-1}$ is
  said to have an inverted gamma distribution of type II,
  $Y\sim IG_2(\nu, s)$.
\end{definition}

\begin{proposition}\label{InvertedGamma2Density}
  The probability density function function of $Y\sim IG_2(\nu, s)$ is:
  \[
    f(y) = \mathcal C\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}y^{-\frac{\nu}{2}-1}e^{-\frac{s}{2 y}}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGamma2Density}]
  Direct from proposition \ref{InvertedGammaDensity} with $\alpha=\nicefrac{\nu}{2}$ and $\beta=\nicefrac{s}{2}$.
\end{proof}

\begin{proposition}\label{InvertedGamma2Moments}
  The expectation and variance of $Y\sim IG_2(\nu, s)$ are:
  \[
    \begin{split}
      \mu &= \frac{s}{\nu-2}\\
      \sigma^2 &= \frac{2\mu^2}{\nu-4}
    \end{split}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGamma2Moments}]
  Direct from proposition \ref{InvertedGammaMoments} with $\alpha=\nicefrac{\nu}{2}$ and $\beta=\nicefrac{s}{2}$.
\end{proof}

The inverted gamma distribution of type II is implemented in \Dynare\ as a prior, using the
keyword \verb+INV_GAMMA2_PDF+. The user has to specify $\mu$ and $\sigma$, and \Dynare\ solves the
two equations given in proposition \ref{InvertedGamma2Moments} for the scale and shape parameters:

\begin{equation}
  \begin{split}
    s &= 2\mu\left(1+\frac{\mu^2}{\sigma^2}\right)\\
    \nu &= 2\left(2+\frac{\mu^2}{\sigma^2}\right)
  \end{split}
\end{equation}

This distribution is often used as a prior for the variance of a structural shock
or measurement error. Note that the sole difference between an inverted gamma
distribution and the inverted gamma distribution of type II is in the parameterization of the
shape and scale parameters. If the prior distribution is defined by its first and
second moments, this difference does not matter.\newline

\begin{proposition}\label{InvertedGamma2Mode}
  The mode of $Y\sim IG_2(\nu, s)$ are:
  \[
    m = \frac{s}{\nu+2}
  \]
\end{proposition}

Note that the mode is strictly positive and smaller than the expectation.

\begin{proof}[Proof of proposition \ref{InvertedGamma2Moments}]
  Direct from proposition \ref{InvertedGammaMode} with $\alpha=\nicefrac{\nu}{2}$ and $\beta=\nicefrac{s}{2}$.
\end{proof}

One can define the prior using the mode and the variance (or alternatively the
mode and the mean). Substituting the expression for the mode in the expression
for $\sigma^2$ and rearranging we obtain the following cubic equation for $\nu$:

\begin{equation}\label{InvertedGamma2Cubic}
\varphi_{\delta}(\nu) \equiv \nu^3 - (8+\delta)\nu^2 + (20-4\delta)\nu - 16 -4\delta = 0
\end{equation}
where $\delta = 2\left(\nicefrac{m}{\sigma}\right)^2$.

\begin{proposition}\label{InvertedGamma2SolveCubic}
  The cubic equation \eqref{InvertedGamma2Cubic} has only one real solution greater than four.
\end{proposition}

Let $\nu^{\star}(\delta)$ be the pertinent root of $\varphi$. We then have $s^{\star} = m(\nu^{\star}(\delta)+2)$ by inverting
the mode formula given in proposition \ref{InvertedGamma2Mode}.

\begin{proof}[Proof of proposition \ref{InvertedGamma2SolveCubic}]
  Let $\bar\nu_1(\delta)$ and $\bar\nu_2(\delta)$ be the roots of the
  second order polynomial $\varphi_{\delta}'(\nu)$. These roots are
  given by:
  \[
    \bar\nu_1(\delta) = \frac{16+2\delta-2\sqrt{\delta^2+28\delta+4}}{6}\leq 2 \quad \forall \delta\geq 0
  \]
  and
  \[
    \bar\nu_2(\delta) = \frac{16+2\delta+2\sqrt{\delta^2+28\delta+4}}{6}\geq \frac{10}{3} \quad \forall \delta\geq 0
  \]
  Note that, since the coefficient associated to $\nu^2$ in
  $\varphi_{\delta}'(\nu)$ is equal to 3, $\varphi_{\delta}'(\nu)$ is
  negative if and only if $\nu$ is betwween the two roots.  Let
  $\tilde\nu(\delta) = \frac{8}{3}+\frac{\delta}{3}$ be the root of
  $\varphi_{\delta}''(\nu)$, we must have:
  \[
    \bar\nu_1(\delta) < \tilde\nu(\delta) < \bar\nu_2(\delta)
  \]
  for any admissible value of $\delta$. As a consequence we have $\varphi_{\delta}'(\tilde\nu(\delta))<0$, and because:
  \[
    \varphi_{\delta}(\tilde\nu(\delta)) = -(8+\delta)\frac{68\delta+\delta^2}{27}-\frac{16}{27}-\frac{56}{27}\delta <0 \quad \forall \delta
  \]
  we necessarily have $\varphi_{\delta}(\bar\nu_2(\delta))<0$. Knowing
  that $\varphi_{\delta}(\nu)$ is monotone increasing in
  $[\bar\nu_2(\delta), \infty)$ and
  $\varphi_{\delta}(4) = -36\delta \leq 0$, the biggest root of the
  third order polynomial has to be greater than four.
\end{proof}

In practice we instead usually define the priors over standard
deviations, i.e. the square root of the variance. This motivates the
following definition.\newline

\begin{definition}\label{InvertedGamma1}
  Let $X>0$ be a real random variable with gamma distribution of type
  I, $X\sim G_1\left(\nu,s\right)$. $Y = X^{-1}$ is said to have an
  inverted gamma distribution of type I, $Y\sim IG_1(\nu, s)$.
\end{definition}

\begin{proposition}\label{InvertedGamma1Density}
  The probability density function function of $Y\sim IG_1(\nu, s)$ is:
  \[
    f(y) = \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}y^{-\nu-1}e^{-\frac{s}{2 y^2}}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGamma1Density}]
  Let $X$ be distributed as a gamma of type I, and $f_X(x)$ be its
  probability density function. Define $h(x) = \nicefrac{1}{x}$. The
  density of the inverted gamma distribution of type I is defined as
  follows:
  \[
    \begin{split}
      f(y) &= f_X\left(h^{-1}(y)\right) \times \left|\frac{\mathrm d}{\mathrm dy}h^{-1}(y)\right|\\
      &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}y^{-\nu+1}e^{-\frac{s}{2 y^2}}\frac{1}{y^2}\\
      &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}y^{-\nu-1}e^{-\frac{s}{2 y^2}}\\
    \end{split}
  \]
\end{proof}

\begin{proposition}\label{InvertedGamma1Moments}
  The expectation and variance of $Y\sim IG_1(\nu, s)$ are:
  \[
    \begin{split}
      \mu &= \sqrt{\frac{s}{2}}\frac{\Gamma\left(\frac{\nu-1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\\
      \sigma^2 &= \frac{s}{\nu-2}-\mu^2
    \end{split}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGamma1Moments}]
  The first order moment is defined by:
  \[
    \begin{split}
      \mu &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1} \int_0^{\infty}yy^{-\nu-1}e^{-\frac{s}{2y^2}}\mathrm dy\\
      &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}\frac{1}{2} \int_0^{\infty} x^{\frac{\nu}{2}-\frac{1}{2}-1}e^{-\frac{s}{2}x}\mathrm d x\\
      &= \mathcal C\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1} \int_0^{\infty} x^{\frac{\nu-1}{2}-1}e^{-\frac{s}{2}x}\mathrm d x\\
      &= \frac{\Gamma(\frac{\nu-1}{2})\left(\frac{2}{s}\right)^{\frac{\nu-1}{2}}}{\Gamma(\frac{\nu}{2})\left(\frac{2}{s}\right)^{\frac{\nu}{2}}}\\
      &= \sqrt{\frac{s}{2}}\frac{\Gamma(\frac{\nu-1}{2})}{\Gamma(\frac{\nu}{2})}
    \end{split}
  \]
  The second order raw moment is defined by:
  \[
    \begin{split}
      \mathbb E \left[Y^2\right] &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1} \int_0^{\infty}y^2y^{-\nu-1}e^{-\frac{s}{2y^2}}\mathrm dy\\
      &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1} \int_0^{\infty}y^{-\nu+1}e^{-\frac{s}{2y^2}}\mathrm dy\\
      &= \widetilde{\mathcal C}\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1}\frac{1}{2} \int_0^{\infty}x^{-\frac{\nu}{2}-2}e^{-\frac{s}{2}x}\mathrm dx\\
      &= \mathcal C\left(\frac{\nu}{2}, \frac{2}{s}\right)^{-1} \int_0^{\infty}x^{-\frac{\nu}{2}-2}e^{-\frac{s}{2}x}\mathrm dx\\
      &= \frac{\Gamma\left(\frac{\nu}{2}-1\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}-1}}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{2}{s}\right)^{\frac{\nu}{2}}}\\
      &= \frac{s}{2} \frac{\Gamma\left(\frac{\nu}{2}-1\right)} {\frac{\nu-2}{2}\Gamma\left(\frac{\nu}{2}-1\right)}\\
      &= \frac{s}{\nu-2}
    \end{split}
  \]
\end{proof}

The inverted gamma distribution of type I is implemented in \Dynare\
as a prior (using the keywords \verb+INV_GAMMA1_PDF+ or
\verb+INV_GAMMA_PDF+). The user has to specify $\mu$ and $\sigma$, and
\Dynare\ solves the equations given in proposition
\ref{InvertedGamma1Moments} for the scale and shape parameters $\nu$
and $s$. There is no closed form solution in this case, a numerical
approach is used. \Dynare\ first solves the following equation for $\nu$:
\[
2\Gamma\left(\frac{\nu}{2}\right)^2\mu^2 = (\sigma^2 + \mu^2)(\nu-2)\Gamma\left(\frac{\nu-1}{2}\right)^2
\]
and then computes
\[
s = (\sigma^2 + \mu^2)(\nu-2)
\]
This is done in the Matlab function \verb+inverse_gamma_specification.m+. Note that in the case
of an infinite variance we have a closed form solution for $\nu$ and $s$:
\[
  \begin{split}
    \nu = 2\\
    s = \frac{2}{\pi}\mu^2
  \end{split}
\]

\begin{proposition}\label{InvertedGamma1Mode}
  The mode of $Y\sim IG_1(\nu, s)$ is:
  \[
    m = \sqrt{\frac{\nu-1}{s}}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{InvertedGamma1Mode}]
  We have:
  \[
    \frac{\mathrm d}{\mathrm d y} f(y) \propto y^{\nu}e^{-\frac{s}{2}y^2}\left(\frac{\nu-1}{y^2}-s\right)
  \]
  which is zero if and only if $y = m$.
\end{proof}

Again one can define the prior using the mode and the
variance. Substituting the expression for the mode in the expression
for $\sigma^2$ and rearranging we obtain:
\[
\sigma^2m^2 = \frac{\nu-1}{\nu-2} - \frac{\nu-1}{2} \left(\frac{\Gamma\left(\frac{\nu-1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\right)^2
\]
\Dynare\ numerically solves this equation for $\nu$ and computes
$s = \nicefrac{\nu-1}{m^2}$. Note that in the case of an infinite
variance we have a closed form solution:
\[
  \begin{split}
    \nu &= 2\\
    s &= \frac{1}{m^2}
  \end{split}
\]

The inverse gamma distribution of type I (type II) is usually used as
a prior for the standard deviation (resp. variance) of a structural
(or measurement) shock. The rational is that in linear models with
gaussian noise, the Normal (for the parameters) – Inverse Gamma
(for the variance of the error) prior is conjugate. Obviously this is
not true for DSGE models, there is no computational advantage in
choosing the inverse gamma prior.

\subsection{Beta distributions}\label{sec:BetaDistributions}

\begin{definition}\label{BetaFunction}
  The beta function is defined as follows:
  \[
    B(\alpha, \beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\mathrm dx
  \]
  with $(\alpha,\beta)\in\mathbb R_+^2$.
\end{definition}

\begin{proposition}\label{BetaAndGammaFunctions}
  The beta function can be written using the gamma function:
  \[
    B(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{BetaAndGammaFunctions}]
  From the definition of the gamma function
  \[
    \begin{split}
      \Gamma(\alpha)\Gamma(\beta) &= \int_{0}^{\infty}\ e^{-x} x^{\alpha-1}\mathrm dx \times \int_{0}^{\infty}\ e^{-y} y^{\beta-1}\mathrm dy \\
      &= \int_{0}^{\infty}\int_{0}^{\infty}  \ e^{-x-y} x^{\alpha-1}y^{\beta-1}\mathrm dx\mathrm dy
    \end{split}
  \]
  We consider the following change of variables:
  \[
    \begin{pmatrix}
      x\\
      y
    \end{pmatrix}
    =
    \begin{pmatrix}
      z t \\
      z (1-t)
    \end{pmatrix}
    \equiv \varphi(z, t)
  \]
  with $t\in[0,1]$. The jacobian matrix associated to this change of variables is:
  \[
    J_{\varphi}(z,t) =
    \begin{pmatrix}
      t & z \\
      (1-t) & -z
    \end{pmatrix}
  \]
  and its determinant:
  \[
    \left|J_{\varphi}(z,t)\right| = -tz-z(1-t) = -z
  \]
  So that we have:
  \[
    \begin{split}
      \Gamma(\alpha)\Gamma(\beta) &= \int_{0}^{\infty}\int_{0}^{1}  \ e^{-z} (tz)^{\alpha-1}\left((1-t)z\right)^{\beta-1}z\mathrm dt\mathrm dz \\
      &= \int_{0}^{\infty} \ e^{-z} z^{\alpha+\beta-1}\mathrm dz \int_{0}^{1} t^{\alpha}(1-t)^{\beta}\mathrm dt\\
      &= \Gamma(\alpha+\beta)B(\alpha,\beta)
    \end{split}
  \]
  \end{proof}

\begin{proposition}\label{BetaDensity}
  Let $X\sim G(\alpha, \gamma)$ and $Y\sim G(\beta, \gamma)$ be two independent
  random variables.The random variable $Z = \nicefrac{X}{X+Y}$ is said
  to be beta distributed, $Z\sim \mathcal B(\alpha,\beta)$,and its
  density function is given by:
  \[
    f(z)=B(\alpha,\beta)^{-1}z^{\alpha-1}(1-z)^{\beta-1}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{BetaDensity}]
  Let $S = X+Y$ be a random variable. We will characterise the joint
  density of $(S,Z)$ and show that these two random variables are
  independent. The joint density of $(X,Y)$ is given by:
  \[
    \begin{split}
      f_{X,Y}(x,y) &= \frac{\gamma^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\gamma x} \times \frac{\gamma^{\beta}}{\Gamma(\beta)}y^{\beta-1}e^{-\gamma y}\\
      &= \frac{\gamma^{\alpha+\beta}}{\Gamma(\alpha)\Gamma(\beta)}e^{-\gamma (x+y)}x^{\alpha-1}y^{\beta-1}\\
      &= \frac{\gamma^{\alpha+\beta}}{\Gamma(\alpha+\beta)}e^{-\gamma (x+y)}B(\alpha,\beta)^{-1}x^{\alpha-1}y^{\beta-1}
    \end{split}
  \]
  because $X$ and $Y$ are independent random variables, and using proposition \ref{BetaAndGammaFunctions}. By
  construction, since $X$ and $Y$ can take values on the positive real
  line, the random variables $S$ and $Z$ are respectively defined over
  $[0,\infty)$ and $[0,1]$. The function mapping $(x,y)$ to $(s,z)$,
  denoted $\varphi(x,y)$, can be inverted as follows:
  \[
    \begin{pmatrix}
      x\\
      y
    \end{pmatrix}
    =
    \varphi^{-1}(s,z)
    =
    \begin{pmatrix}
      sz\\
      s(1-z)
    \end{pmatrix}
  \]
  so we should be able to identify the densities of a gamma and a beta
  distributions in the last expression for the joint density of
  $(X, Y)$. The jacobian matrix associated to the change of variables
  function $\varphi^{-1}$ is given by:
  \[
    J_{\varphi^{-1}}(x,y) =
    \begin{pmatrix}
      z & s \\
      1-z & -s
    \end{pmatrix}
  \]
  and its determinant is:
  \[
    \left|J_{\varphi^{-1}}(s,z)\right| = -zs-s(1-z) = -s
  \]
  Hence the joint density of $(S,Z)$ is
  \[
    \begin{split}
      f_{S,Z}(s,z) &= \frac{\gamma^{\alpha+\beta}}{\Gamma(\alpha+\beta)}e^{-\gamma s}B(\alpha,\beta)^{-1}(sz)^{\alpha-1}(s(1-z))^{\beta-1}s\\
      &= \frac{\gamma^{\alpha+\beta}}{\Gamma(\alpha+\beta)}e^{-\gamma s}s^{\alpha+\beta-1}B(\alpha,\beta)^{-1}z^{\alpha-1}(1-z)^{\beta-1}
    \end{split}
  \]
  Noting that this joint density is separable with respect to  $s$ and $z$, we have the marginal densities for $S$ and $Z$:
  \[
    f_S(s) = \frac{\gamma^{\alpha+\beta}}{\Gamma(\alpha+\beta)}e^{-\gamma s}s^{\alpha+\beta-1}
  \]
  and
  \[
    f_Z(z) = B(\alpha,\beta)^{-1}z^{\alpha-1}(1-z)^{\beta-1}
  \]
  We recognise $f_S(s)$ as the density of a gamma random variable with
  shape parameter $\alpha+\beta$ and scale parameter $\gamma$,
  $S\sim \Gamma(\alpha+\beta,\gamma)$. The remaining terms correspond
  to the density of the beta distribution.
\end{proof}

\begin{proposition}\label{BetaMoments}
  The expectation and variance of $Z\sim \mathcal B(\alpha, \beta)$ are:
  \[
    \begin{split}
      \mu &= \frac{\alpha}{\alpha+\beta}\\
      \sigma^2 &= \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
    \end{split}
  \]
\end{proposition}

\begin{proof}[Proof of proposition \ref{BetaMoments}]
  The expectation is defined by:
  \[
    \begin{split}
      \mu &= \int_0^1 B(\alpha,\beta)^{-1}z^{\alpha}(1-z)^{\beta-1}\mathrm dz\\
      &= B(\alpha,\beta)^{-1}B(\alpha+1,\beta)\\
      &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\alpha\Gamma(\alpha)\Gamma(\beta)}{(\alpha+\beta)\Gamma(\alpha+\beta)}\\
      &= \frac{\alpha}{\alpha+\beta}
    \end{split}
  \]
  The raw second order moment is given by:
  \[
    \begin{split}
      \mathbb E[Z^2] &= \int_0^1 B(\alpha,\beta)^{-1}z^{\alpha+1}(1-z)^{\beta-1}\mathrm dz\\
      &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{(\alpha+1)\alpha\Gamma(\alpha)\Gamma(\beta)}{(\alpha+\beta+1)(\alpha+\beta)\Gamma(\alpha+\beta)}\\
      &= \frac{(\alpha+1)\alpha}{(\alpha+\beta+1)(\alpha+\beta)}
    \end{split}
  \]
  So that the variance is:
  \[
    \begin{split}
      \sigma^2 &= \frac{(\alpha+1)\alpha}{(\alpha+\beta+1)(\alpha+\beta)}-\frac{\alpha^2}{(\alpha+\beta)^2}\\
      &= \frac{(\alpha+1)(\alpha+\beta)\alpha-\alpha^2(\alpha+\beta+1)}{(\alpha+\beta+1)(\alpha+\beta)^2}\\
      &= \frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}
    \end{split}
  \]
\end{proof}

We can deduce the hyperparameters, $\alpha$ and $\beta$, of the beta distribution from these two moments. We have:
\[
  \begin{split}
    \alpha &=  (1-\mu)\left(\frac{\mu}{\sigma}\right)^2 - \mu^2\\
    \beta  &= \left(\frac{(1-\mu)\mu}{\sigma^2}-1\right)(1-\mu)
  \end{split}
\]

\begin{proposition}\label{BetaMode}
  If the hyper-parameters $\alpha>1$ and $\beta>1$ the mode of the beta distribution is unique and given by:
  \[
    m = \frac{\alpha-1}{\alpha+\beta-2}
  \]
  If $\alpha=\beta=1$, then the beta distribution collapses to uniform
  distribution and has an infinity of modes. If $\alpha<1$ the beta
  distribution has a mode in zero (the density is not finite in 0). If
  $\beta<1$, the beta distribution has a mode in one (the density is
  not finite in 1). If both hyper-parameters are less than one, the
  beta distribution has two modes in 0 and 1. If $\alpha=1$ and
  $\beta>1$, the density decreases monotonically between 0 and 1, the
  mode is in 0 where the density is finite. If $\beta=1$ and
  $\alpha>1$, the density increases monotonically between 0 and 1, the
  mode is in 1 where the density is finite.
\end{proposition}

\begin{proof}[Proof of proposition \ref{BetaMode}]
  We focus in the case where we have a unique interior mode (other cases are obvious), i.e. $\alpha>1$ and $\beta>1$. We have:
  \[
    \frac{\mathrm d}{\mathrm dz} f(z) \propto z^{\alpha-1}(1-z)^{\beta-1}\left[\frac{\alpha-1}{z}-\frac{\beta-1}{1-a}\right]
  \]
  This derivative is zero if and only if the term between bracket is zero, i.e.
  \[
    \begin{split}
      \frac{z}{1-\alpha} &= \frac{1-z}{\beta-1} \\
      \Leftrightarrow z &= \frac{\alpha-1}{\alpha+\beta-2}
    \end{split}
  \]
\end{proof}

Again, it is possible to compute the hyper-parameters from the mode and variance, solving a cubic equation.

\end{document}
